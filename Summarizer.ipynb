{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrszp9NrLcwnc2cZrHzY2c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahill132009/Machine-Learning-Course-SPBU/blob/main/Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f98Pzn8JBtNW",
        "outputId": "53f29847-4e14-4503-a7e3-d92c1771c088"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')  # one time execution\n",
        "\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.ru import Russian\n",
        "\n",
        "!pip install pymorphy2\n",
        "!pip install compress-fasttext\n",
        "\n",
        "import compress_fasttext\n",
        "small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load(\n",
        "    'https://github.com/avidale/compress-fasttext/releases/download/v0.0.1/ft_freqprune_100K_20K_pq_100.bin'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: compress-fasttext in /usr/local/lib/python3.7/dist-packages (0.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from compress-fasttext) (1.19.5)\n",
            "Requirement already satisfied: gensim>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from compress-fasttext) (3.8.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.8.1->compress-fasttext) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.8.1->compress-fasttext) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.8.1->compress-fasttext) (4.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUoSOpY-BMnO"
      },
      "source": [
        "class Summarizer:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def preprocess(self, text):\n",
        "    '''Returns list of lists of lemmatized words in sentence'''\n",
        "\n",
        "    self.text = text\n",
        "    self.text = sent_tokenize(self.text)\n",
        "    self.lemmatized_sentences = []\n",
        "    nlp = Russian()\n",
        "    for line in self.text:\n",
        "      self.lemmatized_sentences.append([t.lemma_.lower() for t in nlp(line) if \n",
        "                    t.lemma_ not in stopwords.words('russian') and\n",
        "                    t.lemma_.isalpha() and re.search(r'[А-Яа-я]', t.lemma_)])\n",
        "    return self.lemmatized_sentences\n",
        "  \n",
        "  def average_sentence_vector(self, sentences):\n",
        "    '''Returns list of average vectors of the sentences'''\n",
        "\n",
        "    self.sentences = sentences\n",
        "    self.sent_as_vec = []\n",
        "    for line in self.sentences:\n",
        "      plus = 0\n",
        "      for w in line:\n",
        "        try:\n",
        "          plus += small_model[w]\n",
        "        except KeyError:\n",
        "          pass\n",
        "      # Добавляем 1, чтобы избежать деления на 0 тогда,\n",
        "      # когда попадается пустой лист\n",
        "      plus = plus/(len(line)+1)\n",
        "      # Если предложение состояло только из нерусских слов,\n",
        "      # то вместо вектора класса ndarray, получится float,\n",
        "      # что нарушит целостность списка векторов и даст ошибку при fit()\n",
        "      if not isinstance(plus, float):\n",
        "        self.sent_as_vec.append(plus)\n",
        "    return self.sent_as_vec\n",
        "\n",
        "\n",
        "  def fit_clustering(self, list_of_vector_matrices):\n",
        "    '''Returns list of indices of the sentences that are closest to the\n",
        "    centroid of the cluster'''\n",
        "\n",
        "    self.vectors = list_of_vector_matrices\n",
        "    X = self.vectors\n",
        "    cluster = KMeans(n_clusters=int(np.sqrt(len(X))), init='k-means++')\n",
        "    cluster.fit(X)\n",
        "    #???\n",
        "    y_pred = cluster.predict(X)\n",
        "    self.closest, _ = pairwise_distances_argmin_min(cluster.cluster_centers_, X)\n",
        "    return sorted(self.closest)\n",
        "  \n",
        "  def get_summary(self, indices_of_sentences):\n",
        "    '''Returns summary'''\n",
        "    \n",
        "    self.indices_of_sentences = indices_of_sentences\n",
        "    summary =  ' '.join([str(self.text[sen]) for sen in self.indices_of_sentences])\n",
        "    return summary\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YybdKp3WOau8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}